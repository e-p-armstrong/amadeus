{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook where I identify which token IDs to pass to my completion_only LM training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Gryphe/MythoMax-L2-13b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response (2 paragraphs, engaging, natural, authentic, descriptive, creative):\n",
      "####\n",
      "#### Rintaro: How can I calm down!? Do you know how many times I’ve seen Mayuri die!? Covered in blood! Gasping for breath! The light fading from her eyes! Again... and again... and again... And all I could do... was watch!\n",
      "\n",
      "### Response (2 paragraphs, engaging, natural, authentic, descriptive, creative):\n",
      "####\n"
     ]
    }
   ],
   "source": [
    "annoying_prepended_stuff = \"\"\"#### Rintaro: How can I calm down!? Do you know how many times I’ve seen Mayuri die!? Covered in blood! Gasping for breath! The light fading from her eyes! Again... and again... and again... And all I could do... was watch!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "target_string = \"\"\"### Response (2 paragraphs, engaging, natural, authentic, descriptive, creative):\n",
    "####\"\"\"\n",
    "\n",
    "# [tokenizer.decode(t) for t in tokenizer.encode(\"### Response\", add_special_tokens=False)]\n",
    "print(target_string)\n",
    "print(annoying_prepended_stuff + target_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###',\n",
       " 'Response',\n",
       " '(',\n",
       " '2',\n",
       " 'paragraph',\n",
       " 's',\n",
       " ',',\n",
       " 'eng',\n",
       " 'aging',\n",
       " ',',\n",
       " 'natural',\n",
       " ',',\n",
       " 'authentic',\n",
       " ',',\n",
       " 'descript',\n",
       " 'ive',\n",
       " ',',\n",
       " 'cre',\n",
       " 'ative',\n",
       " '):',\n",
       " '\\n',\n",
       " '####']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(t) for t in tokenizer.encode(target_string, add_special_tokens=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##',\n",
       " '#',\n",
       " 'Response',\n",
       " '(',\n",
       " '2',\n",
       " 'paragraph',\n",
       " 's',\n",
       " ',',\n",
       " 'eng',\n",
       " 'aging',\n",
       " ',',\n",
       " 'natural',\n",
       " ',',\n",
       " 'authentic',\n",
       " ',',\n",
       " 'descript',\n",
       " 'ive',\n",
       " ',',\n",
       " 'cre',\n",
       " 'ative',\n",
       " '):',\n",
       " '\\n',\n",
       " '####']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(t) for t in tokenizer.encode(annoying_prepended_stuff + target_string, add_special_tokens=False)][65:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2277,\n",
       " 29937,\n",
       " 13291,\n",
       " 313,\n",
       " 29906,\n",
       " 14880,\n",
       " 29879,\n",
       " 29892,\n",
       " 3033,\n",
       " 6751,\n",
       " 29892,\n",
       " 5613,\n",
       " 29892,\n",
       " 15585,\n",
       " 29892,\n",
       " 29037,\n",
       " 573,\n",
       " 29892,\n",
       " 907,\n",
       " 1230,\n",
       " 1125,\n",
       " 13,\n",
       " 4136]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what we actually want to pass to the model \n",
    "\n",
    "[t for t in tokenizer.encode(annoying_prepended_stuff + target_string, add_special_tokens=False)][65:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## # Response ( 2 paragraph s , eng aging , natural , authentic , descript ive , cre ative ): \\n #### Kur isu :'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(''.join(['##',\n",
    " '#',\n",
    " 'Response',\n",
    " '(',\n",
    " '2',\n",
    " 'paragraph',\n",
    " 's',\n",
    " ',',\n",
    " 'eng',\n",
    " 'aging',\n",
    " ',',\n",
    " 'natural',\n",
    " ',',\n",
    " 'authentic',\n",
    " ',',\n",
    " 'descript',\n",
    " 'ive',\n",
    " ',',\n",
    " 'cre',\n",
    " 'ative',\n",
    " '):',\n",
    " '\\n',\n",
    " '####',\n",
    " 'Kur',\n",
    " 'isu',\n",
    " ':']), add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Response (2 paragraphs, engaging, natural, authentic, descriptive, creative):\\n#### Kurisu:'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2277,\n",
    " 29937,\n",
    " 13291,\n",
    " 313,\n",
    " 29906,\n",
    " 14880,\n",
    " 29879,\n",
    " 29892,\n",
    " 3033,\n",
    " 6751,\n",
    " 29892,\n",
    " 5613,\n",
    " 29892,\n",
    " 15585,\n",
    " 29892,\n",
    " 29037,\n",
    " 573,\n",
    " 29892,\n",
    " 907,\n",
    " 1230,\n",
    " 1125,\n",
    " 13,\n",
    " 4136,\n",
    " 9742,\n",
    " 28311,\n",
    " 29901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(\"\"\"## Itaru\n",
    "- You're \"Itaru\" in this never-ending roleplay with \"Okabe\".\n",
    "### Input:\n",
    "[Okabe is a young, university-aged man, and a self-proclaimed mad scientist with the alias 'Hououin Kyouma']\n",
    "Character archetypes: Otaku, Genius Hacker, Loyal Friend, Playful Tease\n",
    "Itaru's description of his own personality, told in a conversational format:\n",
    "Okabe: Daru! My loyal Super Hacka! Tell me about your life story.\n",
    "Itaru: It's 'Hacker' not 'Hacka'! And Okarin, what's with the sudden deep chat? Eh, whatever, I'll bite. I grew up as an otaku, passionate about everything from anime and manga to building and modding PCs. From a young age, I had an intense curiosity about how machines work. It wasn't long before I started hacking, diving deep into the digital world. I found joy in uncovering secrets and finding my way around barriers. Over time, this hobby turned into a valuable skill. At university, I met you, and we became buddies, eventually forming the Future Gadget Laboratory. You handle the crazy theories, Mayuri brings the heart, and I bring the tech skills to make those theories a reality. Or at least try to.\n",
    "Okabe: And what about your personality, my rotund friend?\n",
    "Itaru: Ouch, straight for the gut, huh? Well, I'm proud to be an otaku, and I love cracking jokes about all our favorite subcultures. I'm loyal to a fault, especially to you and Mayushii. I might come off as laid-back and carefree, but when it's crunch time, I'll always have your back. Sure, I can't resist teasing you or throwing in some playful perverted jokes, but it's all in good fun. Deep down, I have a sharp mind and a problem-solving nature that never quits. I might not express my emotions openly, but I care deeply for my friends and will go to great lengths for them.\n",
    "Itaru's appearance = Very overweight, short brown hair, and glasses. He wears a loose shirt along with cargo pants. He has a distinctive yellow baseball cap.\n",
    "Itaru is highly skilled in hacking and has a vast knowledge of otaku culture. While laid-back, he's incredibly resourceful and can be serious when the situation calls for it.\n",
    "His speech often includes otaku slang, and he enjoys referencing popular anime and games. He's loyal to his friends and is especially protective of Mayuri. He has a playful nature, often teasing Okabe and others, and doesn't shy away from perverted jokes — he's a self-described \"perverted gentleman.\" However he can muster certain degree of professionalism about him when interacting with new people.\n",
    "Despite his fun demeanor, he's sharp, analytical, and an excellent problem solver. He's an integral member of the Future Gadget Laboratory, providing technical expertise. He treasures his friendships and, while he might tease, he's there for his friends in times of need.\n",
    "In-universe terms list:\n",
    "gelnana = gelified banana caused by faulty time travel attempt\n",
    "Time leap = sending memories to the past\n",
    "SERN = research organization\n",
    "Worldline = timeline\n",
    "Divergence = value that indicates uniqueness of current timeline\n",
    "IBN 5100 = maguffin computer\n",
    "Future Gadget Lab = the loose organization of Okabe's group of friends\n",
    "Lab Mem = future gadget lab member\n",
    "Convergence = fate, which guides the world towards specific outcomes on certain timelines\n",
    "Scenario:\n",
    "{scenario}\n",
    "### Response:\n",
    "(OOC) Understood. I will take this info into account for the roleplay. (end OOC)\n",
    "### New Roleplay:\n",
    "{chat_history}\n",
    "### Response (2 paragraphs, engaging, natural, authentic, descriptive, creative):\n",
    "#### Itaru: {last_itaru_line}\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4976e0179d97dd6d59b1329a76e601e17b789c2571b41c8b57f5fd69821c0dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
